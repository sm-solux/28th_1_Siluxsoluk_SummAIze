# -*- coding: utf-8 -*-
"""BERT_sum_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kuZ6WtmxQ2D6Vnm4XiNIA1Nl0jsGBv3Q

###모델 필요요소 다운로드
"""

!pip install gluonnlp==0.8.0
!pip install pandas tqdm
!pip install mxnet
!pip install sentencepiece
!pip install transformers
!pip install torch
!pip install onnxruntime

!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'

import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook

"""##KoBERT모델"""

#Hugging Face를 통한 모델 및 토크나이저 가져오기
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
from transformers import PreTrainedTokenizerFast
from transformers import BartForConditionalGeneration
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

# GPU 사용 시
device = torch.device("cuda:0")

tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')
#tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
#bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
#vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

!pip install bert-extractive-summarizer

text_path = "/content/수사기관의 정보수집에 관한 최신 해외 사례와 시사점.txt"
with open(text_path) as file:
  text=file.read()
text = text.replace('\n', ' ')

text

from summarizer import Summarizer



model = Summarizer()
result = model(text, num_sentences=4)
full = ''.join(result)
print(full)

text = text.replace('\n', ' ')

tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')


raw_input_ids = tokenizer.encode(text)
input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]

summary_ids = model.generate(torch.tensor([input_ids]),     eos_token_id=1) #max_length=512, num_beams=4,
tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)